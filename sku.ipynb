{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKU Dataset\n",
    "\n",
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "%set_env XLA_PYTHON_CLIENT_PREALLOCATE false\n",
    "%set_env CUDA_VISIBLE_DEVICES 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import jax\n",
    "from jax import random\n",
    "from jax import numpy as jnp\n",
    "from jax.scipy import optimize\n",
    "\n",
    "from loss_functions import MAAPE, SMASPE, MASPE\n",
    "from sku_models import arima_run, additive_hw_run\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "sns.set_theme(\n",
    "    context=\"paper\",\n",
    "    style=\"whitegrid\",\n",
    "    font=\"Roboto\",\n",
    "    font_scale=2\n",
    ")\n",
    "\n",
    "KEY = random.key(0)\n",
    "EPS = keras.backend.epsilon()\n",
    "\n",
    "\n",
    "def fft_com(x):\n",
    "  x_fft = np.abs(np.fft.rfft(x))\n",
    "  return np.sum(x_fft * np.linspace(0, 1, len(x_fft))) / np.sum(x_fft[1:])\n",
    "\n",
    "\n",
    "# Keeping only time-series with at least some intermittence\n",
    "DATASET = pl.read_csv(\"./data/sku.csv\").select(\n",
    "    [\n",
    "        \"Scode\",\n",
    "        \"Pcode\",\n",
    "        pl.selectors.matches(r\"Wk\\d+\")\n",
    "    ]\n",
    ").with_columns(\n",
    "    pl.concat_arr(pl.col(r\"^Wk\\d+$\")).alias(\"ts\")\n",
    ").drop(r\"^Wk\\d+$\").with_columns(\n",
    "    intermittence=pl.col(\"ts\").map_elements(\n",
    "        fft_com,\n",
    "        returns_scalar=True,\n",
    "        return_dtype=pl.Float64\n",
    "    )\n",
    ").sort([\"Scode\", \"Pcode\"]).to_dicts()\n",
    "\n",
    "X = jnp.stack(\n",
    "    [\n",
    "        jnp.asarray(row[\"ts\"])\n",
    "        for row in DATASET\n",
    "    ],\n",
    "    dtype=float\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vis = 5\n",
    "n_rows = len(DATASET)\n",
    "step = n_rows // n_vis\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "for row in DATASET[::step]:\n",
    "  label = f\"{row[\"Scode\"]}: {row[\"Pcode\"]}\"\n",
    "  plt.plot(\n",
    "      np.asarray(row[\"ts\"]),\n",
    "      label=label\n",
    "  )\n",
    "plt.ylabel(\"Units Sold\")\n",
    "plt.xlabel(\"Week\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "fig.tight_layout(pad=2)\n",
    "plt.autoscale(tight=True)\n",
    "fig.savefig(\"./plots/sku_timeseries.svg\", transparent=True)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks\n",
    "\n",
    "### Global Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_IDX = 95\n",
    "X_TRAIN, X_TEST = X[:, :SPLIT_IDX], X[:, SPLIT_IDX:]\n",
    "BOUNDS = [\n",
    "    jnp.min(X_TRAIN, axis=-1, keepdims=True),\n",
    "    jnp.max(X_TRAIN, axis=-1, keepdims=True)\n",
    "]\n",
    "DELTA = BOUNDS[1] - BOUNDS[0]\n",
    "\n",
    "losses = {\n",
    "    \"MAE\": keras.losses.MeanAbsoluteError(),\n",
    "    \"MSE\": keras.losses.MeanSquaredError(),\n",
    "    \"MAAPE\": MAAPE(),\n",
    "    \"MASPE\": MASPE(),\n",
    "    \"SMASPE (tight)\": SMASPE(y_minus=BOUNDS[0], y_plus=BOUNDS[1]),\n",
    "    \"SMASPE (loose)\": SMASPE(\n",
    "        y_minus=BOUNDS[0] - 0.05 * DELTA,\n",
    "        y_plus=BOUNDS[1] + 0.05 * DELTA\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def process_sku_results(results: dict[str, any], suffix: str):\n",
    "  # Printed Results\n",
    "  for loss_name, result in results.items():\n",
    "    row = f\"{loss_name} & {result[\"median_epochs\"]}\"\n",
    "    for _, metric_fn in losses.items():\n",
    "      row += f\" & {metric_fn(\n",
    "          X_TEST,\n",
    "          result[\"predictions\"][:, -X_TEST.shape[-1]:]\n",
    "      ):.3}\"\n",
    "    print(row + r\" \\\\\")\n",
    "\n",
    "  # Test Error Matrix\n",
    "  test_err_matrix = jnp.stack([\n",
    "      (X_TEST - results[k][\"predictions\"][:, -X_TEST.shape[-1]:]).flatten()\n",
    "      for k in results\n",
    "  ], -1)\n",
    "\n",
    "  # Boxplots\n",
    "  boxplot_fig = plt.figure(figsize=(12, 6))\n",
    "  plt.boxplot(\n",
    "      test_err_matrix,\n",
    "      tick_labels=results.keys(),\n",
    "  )\n",
    "  plt.xlabel(\"Training Loss Function\")\n",
    "  plt.ylabel(r\"Error Distribution ($y - \\hat{y}$)\")\n",
    "  plt.yscale(\"symlog\", linthresh=1., linscale=1.)\n",
    "  boxplot_fig.tight_layout(pad=2)\n",
    "  boxplot_fig.savefig(\n",
    "      f\"./plots/sku_err_boxplot_{suffix}.svg\",\n",
    "      transparent=True\n",
    "  )\n",
    "  plt.close(boxplot_fig)\n",
    "\n",
    "  boxplot_fig = plt.figure(figsize=(12, 6))\n",
    "  plt.boxplot(\n",
    "      test_err_matrix / jnp.clip(X_TEST.flatten(), EPS, jnp.inf)[:, None],\n",
    "      tick_labels=results.keys(),\n",
    "  )\n",
    "  plt.xlabel(\"Training Loss Function\")\n",
    "  plt.ylabel(r\"Relative Error Distribution ($\\bar{\\delta}$)\")\n",
    "  plt.yscale(\"symlog\", linthresh=.1, linscale=1.)\n",
    "  boxplot_fig.tight_layout(pad=2)\n",
    "  boxplot_fig.savefig(\n",
    "      f\"./plots/sku_delta_boxplot_{suffix}.svg\",\n",
    "      transparent=True\n",
    "  )\n",
    "  plt.close(boxplot_fig)\n",
    "\n",
    "  # Histograms\n",
    "  hist_fig, hist_axs = plt.subplots(\n",
    "      nrows=2,\n",
    "      ncols=1,\n",
    "      sharex=True,\n",
    "      sharey=True,\n",
    "      figsize=(12, 12)\n",
    "  )\n",
    "  err_bins = np.logspace(\n",
    "      np.log10(5E-6),\n",
    "      np.log10(5E11),\n",
    "      10\n",
    "  )\n",
    "  ref_bins = np.pad(\n",
    "      np.logspace(0, np.log10(np.max(X_TEST)), 9),\n",
    "      (1, 0),\n",
    "      constant_values=0.\n",
    "  )\n",
    "  for idx, loss_name in enumerate([\"MSE\", \"SMASPE (loose)\"]):\n",
    "    _, _, _, mpbl = hist_axs[idx].hist2d(\n",
    "        x=X_TEST.flatten(),\n",
    "        y=np.abs(test_err_matrix[:, list(results.keys()).index(loss_name)]),\n",
    "        bins=[ref_bins, err_bins],\n",
    "        cmap=\"viridis\",\n",
    "        norm=\"log\"\n",
    "    )\n",
    "    hist_axs[idx].set_ylabel(\n",
    "        r\"$\\left| y - \\hat{y}_\\text{\" + loss_name + r\"} \\right|$\"\n",
    "    )\n",
    "    hist_axs[idx].set_xscale(\"symlog\", linthresh=1., linscale=.5)\n",
    "    hist_axs[idx].set_yscale(\"log\")\n",
    "    hist_axs[idx].set_ylim((1e-6, 1e12))\n",
    "    cbar = plt.colorbar(mpbl, ax=hist_axs[idx])\n",
    "    cbar.ax.set_ylabel(\"Frequency\")\n",
    "  hist_axs[-1].set_xlabel(r\"$y$\")\n",
    "  hist_fig.tight_layout(pad=2)\n",
    "  hist_fig.savefig(\n",
    "      f\"./plots/sku_err_histogram_{suffix}.svg\",\n",
    "      transparent=True\n",
    "  )\n",
    "  plt.close(hist_fig)\n",
    "\n",
    "  # Forecasts\n",
    "  pred_tensor = jnp.stack([results[k][\"predictions\"] for k in results])\n",
    "  eff_length = pred_tensor.shape[-1]\n",
    "  product_abs_err = jnp.sum(\n",
    "      jnp.abs(\n",
    "          test_err_matrix.transpose()[-2:].reshape([-1, *X_TEST.shape])\n",
    "      ),\n",
    "      (0, -1)\n",
    "  )\n",
    "\n",
    "  week_range = np.asarray(range(eff_length)) + (X.shape[-1] - eff_length)\n",
    "\n",
    "  best_idx, worst_idx = [\n",
    "      np.argmin(product_abs_err),\n",
    "      np.argmax(product_abs_err)\n",
    "  ]\n",
    "  best_ref, worst_ref = [\n",
    "      X[best_idx, -eff_length:],\n",
    "      X[worst_idx, -eff_length:]\n",
    "  ]\n",
    "\n",
    "  ex_fig, ex_axs = plt.subplots(\n",
    "      nrows=2,\n",
    "      ncols=1,\n",
    "      sharex=True,\n",
    "      figsize=(12, 12)\n",
    "  )\n",
    "\n",
    "  ex_axs[0].plot(\n",
    "      week_range,\n",
    "      best_ref,\n",
    "      \":\",\n",
    "      label=\"Reference\"\n",
    "  )\n",
    "  ex_axs[0].set_ylim([10, 1E3])\n",
    "  ex_axs[0].axvline(\n",
    "      SPLIT_IDX + 0.5,\n",
    "      linestyle=\":\", color=\"red\"\n",
    "  )\n",
    "\n",
    "  ex_axs[1].plot(\n",
    "      week_range,\n",
    "      worst_ref,\n",
    "      \":\",\n",
    "      label=\"Reference\"\n",
    "  )\n",
    "  ex_axs[1].set_ylim([0, 1E4])\n",
    "  ex_axs[1].axvline(\n",
    "      SPLIT_IDX + 0.5,\n",
    "      linestyle=\"--\", color=\"red\"\n",
    "  )\n",
    "\n",
    "  for loss_name in results:\n",
    "    ex_axs[0].plot(\n",
    "        week_range,\n",
    "        results[loss_name][\"predictions\"][best_idx],\n",
    "        label=loss_name\n",
    "    )\n",
    "    ex_axs[1].plot(\n",
    "        week_range,\n",
    "        results[loss_name][\"predictions\"][worst_idx],\n",
    "        label=loss_name\n",
    "    )\n",
    "\n",
    "  ex_axs[0].legend(loc=\"upper left\")\n",
    "\n",
    "  ex_axs[0].set_yscale(\"symlog\", linthresh=1., linscale=1.)\n",
    "  ex_axs[1].set_yscale(\"symlog\", linthresh=1., linscale=1.)\n",
    "  ex_axs[1].set_xlabel(\"Week\")\n",
    "\n",
    "  ex_axs[0].set_xlim(left=20, right=week_range[-1])\n",
    "  ex_axs[1].set_xlim(left=20, right=week_range[-1])\n",
    "\n",
    "  ex_fig.tight_layout(pad=2)\n",
    "  ex_fig.savefig(\n",
    "      f\"./plots/sku_forecasts_{suffix}.svg\",\n",
    "      transparent=True\n",
    "  )\n",
    "  plt.close(ex_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Seasonal ARIMA\n",
    "\n",
    "Each model is trained as a single sequence to preserve MA coherence. However, since we are studying the one-step-ahead case, the real values of the signal are used during the iterative forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, D, Q = 4, 1, 4\n",
    "\n",
    "\n",
    "def arima_helper(\n",
    "        z_train: jnp.ndarray,\n",
    "        z_test: jnp.ndarray,\n",
    "        loss: keras.Loss) -> tuple[jnp.ndarray, int]:\n",
    "  opt_result = optimize.minimize(\n",
    "      lambda w: loss(\n",
    "          z_train[P + D:],\n",
    "          arima_run(w, z_train, [P, D, Q], len(z_train) - P - D)\n",
    "      ),\n",
    "      jnp.zeros(1 + P + Q + 1),\n",
    "      method=\"BFGS\",\n",
    "      tol=EPS\n",
    "  )\n",
    "\n",
    "  pred = arima_run(\n",
    "      opt_result.x,\n",
    "      jnp.concatenate(\n",
    "          [\n",
    "              z_train,\n",
    "              z_test\n",
    "          ]\n",
    "      ),\n",
    "      [P, D, Q],\n",
    "      len(z_train) + len(z_test) - (P + D)\n",
    "  )\n",
    "  return pred, opt_result.nit\n",
    "\n",
    "\n",
    "results_arima = {}\n",
    "\n",
    "for loss_name, loss_fn in losses.items():\n",
    "  if loss_name == \"MAE\":\n",
    "    continue\n",
    "  print(f\"Training with {loss_name}...\", flush=True)\n",
    "\n",
    "  if isinstance(loss_fn, SMASPE):\n",
    "    predictions, iterations = jax.vmap(\n",
    "        lambda z_train, z_test, ym, yp: arima_helper(\n",
    "            z_train=z_train,\n",
    "            z_test=z_test,\n",
    "            loss=SMASPE(ym, yp)\n",
    "        )\n",
    "    )(\n",
    "        X_TRAIN,\n",
    "        X_TEST,\n",
    "        loss_fn.y_minus[:, 0],\n",
    "        loss_fn.y_plus[:, 0]\n",
    "    )\n",
    "  else:\n",
    "    predictions, iterations = jax.vmap(\n",
    "        partial(arima_helper, loss=loss_fn)\n",
    "    )(z_train=X_TRAIN, z_test=X_TEST)\n",
    "\n",
    "  results_arima[loss_name] = {\n",
    "      \"predictions\": jnp.clip(predictions, 0, jnp.inf),\n",
    "      \"median_epochs\": jnp.median(iterations)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_sku_results(results_arima, \"arima\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holt-Winters (Triple Exponential Smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 4\n",
    "\n",
    "\n",
    "def hw_helper(\n",
    "        z_train: jnp.ndarray,\n",
    "        z_test: jnp.ndarray,\n",
    "        loss: keras.Loss) -> tuple[jnp.ndarray, int]:\n",
    "  opt_result = optimize.minimize(\n",
    "      lambda w: loss(\n",
    "          z_train[3 * M + 1:],\n",
    "          additive_hw_run(w, z_train, M, len(z_train) - (3 * M + 1))\n",
    "      ),\n",
    "      0.5 * jnp.ones(3),\n",
    "      method=\"BFGS\",\n",
    "      tol=EPS\n",
    "  )\n",
    "\n",
    "  pred = additive_hw_run(\n",
    "      opt_result.x,\n",
    "      jnp.concatenate(\n",
    "          [\n",
    "              z_train,\n",
    "              z_test\n",
    "          ]\n",
    "      ),\n",
    "      M,\n",
    "      len(z_train) + len(z_test) - (3 * M + 1)\n",
    "  )\n",
    "\n",
    "  return pred, opt_result.nit\n",
    "\n",
    "\n",
    "results_hw = {}\n",
    "\n",
    "for loss_name, loss_fn in losses.items():\n",
    "  if loss_name == \"MAE\":\n",
    "    continue\n",
    "  print(f\"Training with {loss_name}...\", flush=True)\n",
    "\n",
    "  if isinstance(loss_fn, SMASPE):\n",
    "    predictions, iterations = jax.vmap(\n",
    "        lambda z_train, z_test, ym, yp: hw_helper(\n",
    "            z_train=z_train,\n",
    "            z_test=z_test,\n",
    "            loss=SMASPE(ym, yp)\n",
    "        )\n",
    "    )(\n",
    "        X_TRAIN,\n",
    "        X_TEST,\n",
    "        loss_fn.y_minus[:, 0],\n",
    "        loss_fn.y_plus[:, 0]\n",
    "    )\n",
    "  else:\n",
    "    predictions, iterations = jax.vmap(\n",
    "        partial(hw_helper, loss=loss_fn)\n",
    "    )(z_train=X_TRAIN, z_test=X_TEST)\n",
    "\n",
    "  results_hw[loss_name] = {\n",
    "      \"predictions\": jnp.clip(predictions, 0, jnp.inf),\n",
    "      \"median_epochs\": jnp.median(iterations)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_sku_results(results_hw, \"hw\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
